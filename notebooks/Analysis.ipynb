{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5265fd4-40d7-4cbe-91a4-693fe5d4f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    " \n",
    "class LeNet5:\n",
    "    def __init__(self, learning_rate=0.001, num_classes=33):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        # C1: Convolution layer (6 filtres 5x5)\n",
    "        self.W1 = np.random.randn(6, 3, 5, 5) * 0.1  # 3 canaux pour RGB\n",
    "        self.b1 = np.zeros((6, 1))\n",
    "        # C3: Convolution layer (16 filtres 5x5)\n",
    "        self.W3 = np.random.randn(16, 6, 5, 5) * 0.1\n",
    "        self.b3 = np.zeros((16, 1))\n",
    "        # C5: Fully connected (400 -> 120)\n",
    "        self.W5 = np.random.randn(120, 400) * 0.1\n",
    "        self.b5 = np.zeros((120, 1))\n",
    "        # F6: Fully connected (120 -> 84)\n",
    "        self.W6 = np.random.randn(84, 120) * 0.1\n",
    "        self.b6 = np.zeros((84, 1))\n",
    "        # Output layer (84 -> 33)\n",
    "        self.W7 = np.random.randn(self.num_classes, 84) * 0.1\n",
    "        self.b7 = np.zeros((self.num_classes, 1))\n",
    "\n",
    "        # Adam optimizer variables\n",
    "        self.m_W1, self.v_W1 = np.zeros_like(self.W1), np.zeros_like(self.W1)\n",
    "        self.m_b1, self.v_b1 = np.zeros_like(self.b1), np.zeros_like(self.b1)\n",
    "        self.m_W3, self.v_W3 = np.zeros_like(self.W3), np.zeros_like(self.W3)\n",
    "        self.m_b3, self.v_b3 = np.zeros_like(self.b3), np.zeros_like(self.b3)\n",
    "        self.m_W5, self.v_W5 = np.zeros_like(self.W5), np.zeros_like(self.W5)\n",
    "        self.m_b5, self.v_b5 = np.zeros_like(self.b5), np.zeros_like(self.b5)\n",
    "        self.m_W6, self.v_W6 = np.zeros_like(self.W6), np.zeros_like(self.W6)\n",
    "        self.m_b6, self.v_b6 = np.zeros_like(self.b6), np.zeros_like(self.b6)\n",
    "        self.m_W7, self.v_W7 = np.zeros_like(self.W7), np.zeros_like(self.W7)\n",
    "        self.m_b7, self.v_b7 = np.zeros_like(self.b7), np.zeros_like(self.b7)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "    def convolution2d(self, input_data, kernel, bias, stride=1):\n",
    "        if len(input_data.shape) == 3:\n",
    "            input_data = input_data.reshape(1, *input_data.shape)\n",
    "        batch_size, in_channels, in_height, in_width = input_data.shape\n",
    "        num_filters, _, kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "        out_height = (in_height - kernel_height) // stride + 1\n",
    "        out_width = (in_width - kernel_width) // stride + 1\n",
    "\n",
    "        output = np.zeros((batch_size, num_filters, out_height, out_width))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(num_filters):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        start_i = i * stride\n",
    "                        end_i = start_i + kernel_height\n",
    "                        start_j = j * stride\n",
    "                        end_j = start_j + kernel_width\n",
    "                        region = input_data[b, :, start_i:end_i, start_j:end_j]\n",
    "                        output[b, f, i, j] = np.sum(region * kernel[f]) + bias[f]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def average_pooling(self, input_data, pool_size=2, stride=2):\n",
    "        batch_size, num_filters, in_height, in_width = input_data.shape\n",
    "        out_height = (in_height - pool_size) // stride + 1\n",
    "        out_width = (in_width - pool_size) // stride + 1\n",
    "\n",
    "        output = np.zeros((batch_size, num_filters, out_height, out_width))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(num_filters):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        start_i = i * stride\n",
    "                        end_i = start_i + pool_size\n",
    "                        start_j = j * stride\n",
    "                        end_j = start_j + pool_size\n",
    "                        region = input_data[b, f, start_i:end_i, start_j:end_j]\n",
    "                        output[b, f, i, j] = (np.sum(region * kernel[f]) + bias[f]).item()\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        if len(X.shape) == 3 and X.shape[0] != 3:\n",
    "            X = X.reshape(-1, 3, 32, 32)\n",
    "\n",
    "        self.X_input = X\n",
    "\n",
    "        # C1: Convolution\n",
    "        self.Z1 = self.convolution2d(X, self.W1, self.b1)\n",
    "        self.A1 = self.tanh(self.Z1)\n",
    "\n",
    "        # S2: Average Pooling\n",
    "        self.A2 = self.average_pooling(self.A1)\n",
    "\n",
    "        # C3: Convolution\n",
    "        self.Z3 = self.convolution2d(self.A2, self.W3, self.b3)\n",
    "        self.A3 = self.tanh(self.Z3)\n",
    "\n",
    "        # S4: Average Pooling\n",
    "        self.A4 = self.average_pooling(self.A3)\n",
    "\n",
    "        # Flatten\n",
    "        batch_size = self.A4.shape[0]\n",
    "        self.A4_flat = self.A4.reshape(batch_size, -1).T  # (400, batch_size)\n",
    "\n",
    "        # C5: Fully connected\n",
    "        self.Z5 = np.dot(self.W5, self.A4_flat) + self.b5\n",
    "        self.A5 = self.tanh(self.Z5)\n",
    "\n",
    "        # F6: Fully connected\n",
    "        self.Z6 = np.dot(self.W6, self.A5) + self.b6\n",
    "        self.A6 = self.tanh(self.Z6)\n",
    "\n",
    "        # Output layer\n",
    "        self.Z7 = np.dot(self.W7, self.A6) + self.b7\n",
    "        self.A7 = self.softmax(self.Z7)\n",
    "\n",
    "        return self.A7\n",
    "\n",
    "    def compute_cost(self, A7, Y):\n",
    "        m = Y.shape[1]\n",
    "        cost = -np.sum(Y * np.log(A7 + 1e-8)) / m\n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        A7 = self.forward_propagation(X)\n",
    "        return np.argmax(A7, axis=0)\n",
    "\n",
    "    def backward_propagation(self, X, Y):\n",
    "        m = Y.shape[1]\n",
    "        dZ7 = self.A7 - Y\n",
    "        dW7 = np.dot(dZ7, self.A6.T) / m\n",
    "        db7 = np.sum(dZ7, axis=1, keepdims=True) / m\n",
    "\n",
    "        dA6 = np.dot(self.W7.T, dZ7)\n",
    "        dZ6 = dA6 * self.tanh_derivative(self.Z6)\n",
    "        dW6 = np.dot(dZ6, self.A5.T) / m\n",
    "        db6 = np.sum(dZ6, axis=1, keepdims=True) / m\n",
    "\n",
    "        dA5 = np.dot(self.W6.T, dZ6)\n",
    "        dZ5 = dA5 * self.tanh_derivative(self.Z5)\n",
    "        dW5 = np.dot(dZ5, self.A4_flat.T) / m\n",
    "        db5 = np.sum(dZ5, axis=1, keepdims=True) / m\n",
    "\n",
    "        dA4_flat = np.dot(self.W5.T, dZ5)\n",
    "        dA4 = dA4_flat.T.reshape(self.A4.shape)\n",
    "\n",
    "        dW3 = np.random.randn(*self.W3.shape) * 0.001\n",
    "        db3 = np.random.randn(*self.b3.shape) * 0.001\n",
    "        dW1 = np.random.randn(*self.W1.shape) * 0.001\n",
    "        db1 = np.random.randn(*self.b1.shape) * 0.001\n",
    "\n",
    "        gradients = {\n",
    "            'dW7': dW7, 'db7': db7,\n",
    "            'dW6': dW6, 'db6': db6,\n",
    "            'dW5': dW5, 'db5': db5,\n",
    "            'dW3': dW3, 'db3': db3,\n",
    "            'dW1': dW1, 'db1': db1\n",
    "        }\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters_adam(self, gradients, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        params = ['W1', 'b1', 'W3', 'b3', 'W5', 'b5', 'W6', 'b6', 'W7', 'b7']\n",
    "        for param in params:\n",
    "            m_name = 'm_' + param\n",
    "            v_name = 'v_' + param\n",
    "            grad_name = 'd' + param\n",
    "            setattr(self, m_name, beta1 * getattr(self, m_name) + (1 - beta1) * gradients[grad_name])\n",
    "            setattr(self, v_name, beta2 * getattr(self, v_name) + (1 - beta2) * (gradients[grad_name] ** 2))\n",
    "            m_corrected = getattr(self, m_name) / (1 - beta1 ** t)\n",
    "            v_corrected = getattr(self, v_name) / (1 - beta2 ** t)\n",
    "            setattr(self, param, getattr(self, param) - self.learning_rate * m_corrected / (np.sqrt(v_corrected) + epsilon))\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val, epochs=50, optimizer='adam'):\n",
    "        costs = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            A7 = self.forward_propagation(X_train)\n",
    "            cost = self.compute_cost(A7, Y_train)\n",
    "            costs.append(cost)\n",
    "            gradients = self.backward_propagation(X_train, Y_train)\n",
    "\n",
    "            if optimizer == 'adam':\n",
    "                self.update_parameters_adam(gradients, epoch + 1)\n",
    "\n",
    "            train_pred = self.predict(X_train)\n",
    "            train_acc = np.mean(train_pred == np.argmax(Y_train, axis=0))\n",
    "            train_accuracies.append(train_acc)\n",
    "\n",
    "            val_pred = self.predict(X_val)\n",
    "            val_acc = np.mean(val_pred == np.argmax(Y_val, axis=0))\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Cost: {cost:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        return costs, train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "# --- Chargement des données avec OpenCV ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Chemin vers le dossier contenant les sous-dossiers de classes\n",
    "data_dir = r\"C:\\Users\\user\\Desktop\\IMSD\\amhcd-data-64\\tifinagh-images\"\n",
    "\n",
    "# Création d'un DataFrame pour stocker les chemins et labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label, class_folder in enumerate(os.listdir(data_dir)):\n",
    "    class_path = os.path.join(data_dir, class_folder)\n",
    "    if os.path.isdir(class_path):\n",
    "        for img_file in os.listdir(class_path):\n",
    "            image_paths.append(os.path.join(class_folder, img_file))\n",
    "            labels.append(label)\n",
    "\n",
    "labels_df = pd.DataFrame({'image_path': image_paths, 'label_encoded': labels})\n",
    "\n",
    "# Encoder les noms de classes\n",
    "label_encoder = LabelEncoder()\n",
    "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['image_path'].apply(lambda x: x.split(os.sep)[0]))\n",
    "\n",
    "# Fonction pour charger et prétraiter une image\n",
    "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    return img.transpose(2, 0, 1)  # Convertir en format (C, H, W)\n",
    "\n",
    "# Charger toutes les images\n",
    "X = np.array([load_and_preprocess_image(os.path.join(data_dir, path)) for path in labels_df['image_path']])\n",
    "y = labels_df['label_encoded'].values\n",
    "\n",
    "# Diviser en ensembles d’entraînement, validation et test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Convertir explicitement en NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == X.shape[0], \"Train-val-test split sizes must sum to total samples\"\n",
    "print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Encoder les étiquettes en one-hot\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = np.array(one_hot_encoder.fit_transform(y_train.reshape(-1, 1)))\n",
    "y_val_one_hot = np.array(one_hot_encoder.transform(y_val.reshape(-1, 1)))\n",
    "y_test_one_hot = np.array(one_hot_encoder.transform(y_test.reshape(-1, 1)))\n",
    "\n",
    "# Création du modèle\n",
    "num_classes = len(np.unique(y))\n",
    "model = LeNet5(learning_rate=0.001, num_classes=num_classes)\n",
    "\n",
    "# Entraînement\n",
    "print(\"Début de l'entraînement...\")\n",
    "costs, train_acc, val_acc = model.train(\n",
    "    X_train, y_train_one_hot, X_val, y_val_one_hot,\n",
    "    epochs=50, optimizer='adam'\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "test_pred = model.predict(X_test)\n",
    "test_accuracy = np.mean(test_pred == np.argmax(y_test_one_hot, axis=0))\n",
    "print(f\"Accuracy sur le test set: {test_accuracy:.4f}\")\n",
    "\n",
    "# Rapport de classification\n",
    "print(\"\\nRapport de classification (Test set) :\")\n",
    "print(classification_report(y_test, test_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matrice de Confusion (Test set)')\n",
    "plt.xlabel('Prédiction')\n",
    "plt.ylabel('Vérité')\n",
    "plt.show()\n",
    "\n",
    "# Courbes d'apprentissage\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(costs)\n",
    "ax1.set_title('Courbe de perte')\n",
    "ax1.set_xlabel('Époque')\n",
    "ax1.set_ylabel('Perte')\n",
    "\n",
    "ax2.plot(train_acc, label='Train Accuracy')\n",
    "ax2.plot(val_acc, label='Validation Accuracy')\n",
    "ax2.set_title('Courbes d\\'accuracy')\n",
    "ax2.set_xlabel('Époque')\n",
    "ax2.legend()\n",
    "ax2.set_ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
